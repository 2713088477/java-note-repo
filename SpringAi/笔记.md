# SpringAI

## 课程大纲

![](assets/大纲.png)

## 1.AI的发展

AI，人工智能(Artificial Intelligence),使机器能够像人类一样思考、学习和解决问题的技术。

发展历史:

![](assets/AI历史.png)

在自然语言处理（Natural Language Processing，NLP）中，有一项关键技术叫**Transformer**，这是一种先进的神经网络模型，是现如今AI高速发展的最主要原因。

我们所熟知的大模型（Large Language Models，LLM），例如GPT、DeepSeek底层都是采用Transformer神经网络模型。

由此可见AI高速发展的原因:1.Transformer的神经网络模型  2.计算机算力的提升

![](assets/GTP.png)

## 2.大模型应用

### 模型部署

**云部署**:

优点:前期成本低，部署维护简单，弹性扩展，全球访问

缺点:数据隐私，网络依赖，长期成本高

**开放API**

优点:前期成本极低，无需部署，无需维护，全球访问

缺点:数据隐私，网络依赖，长期成本高，定制限制

**本地部署:**

优点:数据安全，不依赖外部网络，长期成本低，高度定制

缺点:初始成本高，维护复杂，部署周期长

### 本地部署

本地部署最简单的一种方案就是使用ollama,官网地址: https://ollama.com/

ollama是一个模型管理工具和平台，它提供了很多国内外常见的模型，功能类似于docker

安装之后，会默认启动一个服务，可以在控制台中输入命令

![](assets/ollama命令.png)

### 调用大模型

以下是DeekSeek官方给出的一段API示例代码:

```python
from openai import OpenAI

# 1. 初始化OpenAI客户端
client = OpenAI(api_key="<DeepSeek API Key>", base_url="https://api.deepseek.com")

# 2. 发送http请求到大模型
response = client.chat.completions.create(
    model="deepseek-r1",
    messages=[
        {"role": "system", "content": "你是一个热心的AI助手，你的名字叫小团团"},
        {"role": "user", "content": "你好，你是谁？"}
    ],
    stream=False
)

# 3. 打印返回结果
print(response.choices[0].message.content)
```

给本地的大模型发送http请求

![](assets/发送http请求.png)

### 大模型应用

大模型应用是基于大模型的推理、分析、生成能力，结合传统编程能力，开发出的各种应用。

![](assets/传统模型和AI大模型对比.png)

![](assets/大模型应用.png)

![](assets/大模型应用领域.png)

### 大模型应用开发技术方案

![](assets/AI应用开发技术架构.png)

#### 1.纯Prompt问答

特征:利用大模型推理能力完成应用的核心功能

应用场景:

1.文本摘要分析

2.舆情分析

3.坐席检查

4.AI对话

![](assets/Prompt问答.png)

#### 2.Function Calling

特征:将应用端业务能力与AI大模型推理能力结合，简化复杂业务功能开发

应用场景:

1. 旅行指南

2. 数据提取

3. 数据聚合分析

4. 课程顾问

![](assets/FunctionCalling.png)

#### 3.RAG Embeddings

离线步骤:

1. 文档加载

2. 文档切分

3. 文档编码

4. 写入知识库

在线步骤:

1. 获取用户问题

2. 检索知识库中相关知识片段

3. 将检索结果和用户问题填入Prompt模板

4. 用最终获得的Prompt调用LLM

5. 由LLM生成回复

应用场景:

1. 个人知识库

2. AI客服助手

![](assets/RAG%20Embedding.png)

## 3.Spring AI

**使用步骤:**

1.引入依赖

```xml
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.springframework.ai</groupId>
            <artifactId>spring-ai-bom</artifactId>
            <version>${spring-ai.version}</version>
            <type>pom</type>
            <scope>import</scope>
        </dependency>
    </dependencies>
</dependencyManagement>
```

```xml
<dependency>
    <groupId>org.springframework.ai</groupId>
    <artifactId>spring-ai-ollama-spring-boot-starter</artifactId>
</dependency>
```

2.配置模型

以本地ollama为例:

```yml
spring:
  ai:
    ollama:
      base-url: http://localhost:11434
      chat:
        model: deepseek-r1:7b
```

以OpenAI为例:

```yml
spring:
  ai:
    openai:
      base-url: https://dashscope.aliyuncs.com/compatible-mode
      api-key: ${OPENAI_API_KEY}
      chat:
        options:
          model: qwen-max  # 模型名称
          temperature: 0.8  # 模型温度，值越大，输出结果越随机
```

3.配置客户端

```java
@Bean
public ChatClient chatClient(OllamaChatModel model) {
    return ChatClient.builder(model)
            .defaultSystem("你是可爱的助手，名字叫小团团")
            .build();
}
```

```java
String content = chatClient.prompt()
        .user("你是谁？")
        .call()//阻塞调用
        .content();
```

```java
Flux<String> content = chatClient.prompt()
        .user("你是谁？")
        .stream()//流式调用
        .content();
```

```java
@RequiredArgsConstructor
@RestController
@RequestMapping("/ai")
public class ChatController {
    private final ChatClient chatClient;

    @GetMapping("/chatBlod")
    public String chatBlod(String prompt){
        return chatClient.prompt()
                .user(prompt)
                .call()
                .content();
    }
    @GetMapping(value = "/chatFlux",produces = "text/html;charset=utf-8")
    public Flux<String> chatFlux(String prompt){
        return chatClient.prompt()
                .user(prompt)
                .stream()
                .content();
    }

}
```

### 会话日志

Spring AI利用AOP原理提供了AI会话的拦截、增强等功能，也就是Advisor

![](assets/Advisor.png)

实现起来很简单:

1.配置一个环绕通知

```java
@Configuration
public class CommonConfiguration {
    @Bean
    public ChatClient chatClient(OllamaChatModel model){
        return   ChatClient
                .builder(model)
                .defaultSystem("你是一个热心的人工智能助手，名字是苏无名")
                .defaultAdvisors(new SimpleLoggerAdvisor())//配置Advisor
                .build();
    }
}
```

2.设置日志级别

```yml
logging:
  level:
    org.springframework.ai.chat.client.advisor: debug
    com.hao.springaiproject: debug
```


